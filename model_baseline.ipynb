{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a808d56b",
   "metadata": {},
   "source": [
    "## Baseline Model Without Active Learning or Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2995e93",
   "metadata": {},
   "source": [
    "To see what kind of performance our model achieves without any label querying or data augmentation, let's collect a random sample of data points, and see what accuracy we can achieve on the test set. For simplicity, we choose the size of the random sample to be 10,000 (~17% of the data). \n",
    "\n",
    "This baseline will help us have a rough sense of how much lift active learning and data augmentation add to our model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4470eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Concatenate\n",
    ")\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from data.get_data import load_mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1576da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the data\n",
    "X_train, y_train = load_mnist('data/f_mnist_data', kind='train')\n",
    "X_test, y_test = load_mnist('data/f_mnist_data', kind='t10k')\n",
    "\n",
    "# minmax scaling\n",
    "X_train, X_test = X_train/255.0, X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b512e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# assemble random sample of data\n",
    "sample_size = 10000\n",
    "random_sample = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
    "X_train_sample = X_train[random_sample]\n",
    "y_train_sample = y_train[random_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18e7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "\n",
    "    # model configs and hyperparams\n",
    "    dim = (28,28,1)\n",
    "    dropout_rate = .25\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=dim),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),      \n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),   \n",
    "        MaxPooling2D(pool_size=(2, 2)),   \n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    adam = Adam(lr=0.001, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "classifier = KerasClassifier(create_keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2ff23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 22:10:21.056574: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-28 22:10:21.211764: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.4301 - accuracy: 0.6055\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.4866 - accuracy: 0.8261\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.4317 - accuracy: 0.8382\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.3807 - accuracy: 0.8623\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.3456 - accuracy: 0.8709\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.3153 - accuracy: 0.8796\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.2887 - accuracy: 0.8936\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2737 - accuracy: 0.9028\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 26s 83ms/step - loss: 0.2701 - accuracy: 0.9025\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 27s 86ms/step - loss: 0.2404 - accuracy: 0.9072\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 0.2549 - accuracy: 0.9086\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.2101 - accuracy: 0.9255\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 27s 85ms/step - loss: 0.2343 - accuracy: 0.9154\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 26s 84ms/step - loss: 0.2072 - accuracy: 0.9256\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 26s 82ms/step - loss: 0.2204 - accuracy: 0.9222\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.2050 - accuracy: 0.9241\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 26s 83ms/step - loss: 0.1895 - accuracy: 0.9354\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 26s 84ms/step - loss: 0.1787 - accuracy: 0.9337\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 26s 84ms/step - loss: 0.1810 - accuracy: 0.9372\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2063 - accuracy: 0.9319\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.1477 - accuracy: 0.9469\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.1649 - accuracy: 0.9436\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.1613 - accuracy: 0.9450\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 26s 82ms/step - loss: 0.1335 - accuracy: 0.9523\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.1401 - accuracy: 0.9500\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.1507 - accuracy: 0.9507\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.1662 - accuracy: 0.9408\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.1261 - accuracy: 0.9562\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.1292 - accuracy: 0.9553\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.1223 - accuracy: 0.9549\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.1181 - accuracy: 0.9611\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1506 - accuracy: 0.9523\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1181 - accuracy: 0.9562\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.1088 - accuracy: 0.9629\n",
      "Epoch 35/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1028 - accuracy: 0.9655\n",
      "Epoch 36/40\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.1175 - accuracy: 0.9600\n",
      "Epoch 37/40\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.1128 - accuracy: 0.9621\n",
      "Epoch 38/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1076 - accuracy: 0.9618\n",
      "Epoch 39/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1346 - accuracy: 0.9592\n",
      "Epoch 40/40\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0825 - accuracy: 0.9719\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6450 - accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "epochs = 40\n",
    "\n",
    "# fit model\n",
    "results = classifier.fit(X_train_sample, y_train_sample, epochs=epochs)\n",
    "test_acc = classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fcb1c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total samples seen by baseline model during training: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of total samples seen by baseline model during training: {X_train_sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3301d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.8888000249862671\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on Test Set: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9046b",
   "metadata": {},
   "source": [
    "88.88% accuracy is a pretty solid basline to be working with. It's close to the target goal of 90% accuracy and we've only used ~17% of the data. But, if we can pick the examples we use to train our model more intelligently, it's possible that we can achieve even greater accuracy - and cross our 90% accuracy threshold - using fewer data points.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
